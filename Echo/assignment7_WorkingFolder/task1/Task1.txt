1.1 Get a feeling for similarity as a human
===========================================
D1 and D2 are most similar set of two documents to each other between these three documents, because they have more common words in their documents than any other two documents.
----------------------------------------------------------------
1.2 Model the documents as vectors and use the cosine similarity
================================================================
1. How many base vectors would be needed to model the documents of this corpus?
answer: 19 base vectors, for each unique word in the corpus
----------------------------------------------------------------
2. What does each dimension of the vector space stand for?
answer: Each dimension represents each word in the corpus. The document is drawn in regards of the term frequency of that word in its document. For example, a dimension of w1 is there and the document vector (for example D1) uses the term frequency of w1 in its document which equals to 1.
----------------------------------------------------------------
3. How many dimensions does the vector space have?
answer: 19 dimensions
----------------------------------------------------------------
4. Create a table to map words of the documents to the base vectors.
answer: words vectors
																					using tfidf model:
(	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	)	vector(w1) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	)	vector(w2) = log2(3/2) = 0.58
(	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	)	vector(w3) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	)	vector(w4) = log2(3/2) = 0.58
(	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	)	vector(w5) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	)	vector(w6) = log2(3/2) = 0.58
(	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	)	vector(w7) = log2(3/2) = 0.58
(	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	)	vector(w8) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	)	vector(w9) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	)	vector(w10) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	)	vector(w11) = log2(3/1) = 1.58
(	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	)	vector(w12) = log2(3/1) = 1.58
(	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w13) = log2(3/1) = 1.58
(	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w14) = log2(3/1) = 1.58
(	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w15) = log2(3/1) = 1.58
(	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w16) = log2(3/1) = 1.58
(	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w17) = log2(3/1) = 1.58
(	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w18) = log2(3/1) = 1.58
(	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	)	vector(w19) = log2(3/1) = 1.58
----------------------------------------------------------------------------------------------------------
5. Use the notation and formulas from the lecture to represent the documents as document vectors in the word vector space. You can use the term frequency of the words as coefcients. You can / should omit the inverse document frequency.
answer:
D1 = w1,w2,w3,w4,w5,w6,w7
D2 = w6,w7,w2,w8,w9,w10,w11,w4,w12
D3 = w13,w14,w15,w16,w17,w18,w19
using term frequency model:
				w19	w18	w17	w16	w15	w14	w13	w12	w11	w10	w9	w8	w7	w6	w5	w4	w3	w2	w1
vector(D1)	(	0	0	0	0	0	0	0	0	0	0	0	0	1	1	1	1	1	1	1	)
vector(D2)	(	0	0	0	0	0	0	0	1	1	1	1	1	1	1	0	1	0	1	0	)
vector(D3)	(	1	1	1	1	1	1	1	0	0	0	0	0	0	0	0	0	0	0	0	)

using the tfidf model:
			w19		w18		w17		w16		w15		w14		w13		w12		w11		w10		w9		w8		w7		w6		w5		w4		w3		w2		w1
vector(D1)(	0		0		0		0		0		0		0		0		0		0		0		0		0.58	0.58	1.58	0.58	1.58	0.58	1.58	)
vector(D2)(	0		0		0		0		0		0		0		1.58	1.58	1.58	1.58	1.58	0.58	0.58	0		0.58	0		0.58	0		)
vector(D3)(	1.58	1.58	1.58	1.58	1.58	1.58	1.58	0		0		0		0		0		0		0		0		0		0		0		0		)
----------------------------------------------------------------------
6. Calculate the cosine similarity between all three pairs of vectors.
answer: using term frequency model:
length(vector(D1)) = sqrt((1^2)*7) = 2.64
length(vector(D2)) = sqrt((1^2)*9) = 3
length(vector(D3)) = sqrt((1^2)*7) = 2.64

similarity(vector(D1), vector(D2)) = (4)/(2.64*3) = 0.50
similarity(vector(D2), vector(D3)) = (0)/(3*2.64) = 0
similarity(vector(D1), vector(D3)) = (0)/(2.64*2.64) = 0

answer: using tfidf model:
length(vector(D1)) = sqrt(0.58^2+0.58^2+1.58^2+0.58^2+1.58^2+0.58^2+1.58^2) = 2.97
length(vector(D2)) = sqrt(1.58^2+1.58^2+1.58^2+1.58^2+1.58^2+0.58^2+0.58^2+0.58^2+0.58^2) = 3.71
length(vector(D3)) = sqrt(1.58^2+1.58^2+1.58^2+1.58^2+1.58^2+1.58^2+1.58^2) = 4.18

similarity(vector(D1), vector(D2)) = 4*(0.58*0.58)/(2.97*3.71) = 0.122
similarity(vector(D2), vector(D3)) = 0/(3.71*4.18) = 0
similarity(vector(D1), vector(D3)) = 0/(2.97*4.18) = 0
------------------------------------------------------------------------------------------------------------
7. According to the cosine similarity which 2 documents are most similar according to the constructed model.
answer:
(D1 and D2) are the most similar set of two documents according to the cosine similarity. Other sets of two documents have the similarity of (0)
------------------------------------------------------------------------------------------------------------
1.3 Discussion
==============
In the calculation of the cosine similarity, if two documents share the same word then the value of the dot product of the two documents is calculated by multiplying the word (tfidf) in the first document by the same word (tfidf) in the second product other than that it'll be a zero field. Therefore, the documents that don't have words in common will end up having a zero dot product.
If we are not using (tfidf), it can be explained as in the document will have a value of (1) in the field of that word and if the second product also has a value of (1) in the field of that word as will, the final result of the dot product won't be zero as in when one of those fields is of a value of (0); when the word is not a common word in both documents.